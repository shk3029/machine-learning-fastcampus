{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/js/Desktop/fastcampus/machine_learning-fastcampus/4주차 앙상블/3. 실습코드'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../2. Data/otto_train.csv\") # Product Category\n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의미가 없다고 판단되는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis = 1) # id 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타겟 변수의 문자열을 숫자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\"Class_1\": 1,\n",
    "                \"Class_2\": 2,\n",
    "                \"Class_3\": 3,\n",
    "                \"Class_4\": 4,\n",
    "                \"Class_5\": 5,\n",
    "                \"Class_6\": 6,\n",
    "                \"Class_7\": 7,\n",
    "                \"Class_8\": 8,\n",
    "                \"Class_9\": 9}\n",
    "after_mapping_target = data['target'].apply(lambda x: mapping_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target'])) # target을 제외한 모든 행\n",
    "X = data[feature_columns] # 설명변수\n",
    "y = after_mapping_target # 타겟변수\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:30:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[23:30:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 76.67 %\n",
      "Time: 7.39 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "import xgboost as xgb\n",
    "import time\n",
    "start = time.time() # 시작 시간 지정\n",
    "xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) # 학습 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data = test_x) # 평가 데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_param = {'max_depth': 10, # 트리 깊이\n",
    "         'learning_rate': 0.01, # Step Size\n",
    "         'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "         'objective': 'multi:softmax', # 목적 함수\n",
    "        'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) # 학습 진행\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 평가 데이터 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 3., 6., ..., 9., 2., 7.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Time: 2.85 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'multiclass', # 목적 함수\n",
    "            'num_class': len(set(train_y)) + 1} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01734061e-15, 2.25081693e-02, 3.62193933e-01, ...,\n",
       "        3.24234521e-02, 5.82126692e-02, 3.67722414e-02],\n",
       "       [1.14084116e-15, 5.36978636e-02, 1.90687128e-01, ...,\n",
       "        3.25081119e-01, 9.38028846e-02, 6.50463131e-02],\n",
       "       [5.94595781e-16, 9.66842220e-03, 5.82817482e-02, ...,\n",
       "        1.42318289e-02, 3.40230275e-02, 2.14919364e-02],\n",
       "       ...,\n",
       "       [7.09105769e-16, 4.63740004e-02, 1.08297559e-01, ...,\n",
       "        5.46934960e-02, 7.24513712e-02, 5.74635996e-01],\n",
       "       [9.88127136e-16, 1.54895684e-02, 5.45515599e-01, ...,\n",
       "        2.45870954e-02, 5.65410617e-02, 3.62344513e-02],\n",
       "       [7.59617500e-16, 1.49480877e-02, 7.44570300e-02, ...,\n",
       "        5.76695793e-01, 1.43227106e-01, 2.74567219e-02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 307ms\tremaining: 30.4s\n",
      "1:\tlearn: 0.6356107\ttotal: 541ms\tremaining: 26.5s\n",
      "2:\tlearn: 0.6411256\ttotal: 826ms\tremaining: 26.7s\n",
      "3:\tlearn: 0.6480344\ttotal: 1.07s\tremaining: 25.6s\n",
      "4:\tlearn: 0.6508222\ttotal: 1.32s\tremaining: 25.1s\n",
      "5:\tlearn: 0.6499939\ttotal: 1.55s\tremaining: 24.3s\n",
      "6:\tlearn: 0.6507818\ttotal: 1.79s\tremaining: 23.8s\n",
      "7:\tlearn: 0.6548422\ttotal: 2.03s\tremaining: 23.4s\n",
      "8:\tlearn: 0.6559533\ttotal: 2.28s\tremaining: 23s\n",
      "9:\tlearn: 0.6560947\ttotal: 2.54s\tremaining: 22.8s\n",
      "10:\tlearn: 0.6568421\ttotal: 2.77s\tremaining: 22.5s\n",
      "11:\tlearn: 0.6588219\ttotal: 3.01s\tremaining: 22.1s\n",
      "12:\tlearn: 0.6592259\ttotal: 3.24s\tremaining: 21.7s\n",
      "13:\tlearn: 0.6611248\ttotal: 3.49s\tremaining: 21.4s\n",
      "14:\tlearn: 0.6625591\ttotal: 3.72s\tremaining: 21.1s\n",
      "15:\tlearn: 0.6631853\ttotal: 3.95s\tremaining: 20.8s\n",
      "16:\tlearn: 0.6639328\ttotal: 4.21s\tremaining: 20.6s\n",
      "17:\tlearn: 0.6668821\ttotal: 4.46s\tremaining: 20.3s\n",
      "18:\tlearn: 0.6669630\ttotal: 4.69s\tremaining: 20s\n",
      "19:\tlearn: 0.6675286\ttotal: 4.92s\tremaining: 19.7s\n",
      "20:\tlearn: 0.6673266\ttotal: 5.17s\tremaining: 19.5s\n",
      "21:\tlearn: 0.6677104\ttotal: 5.42s\tremaining: 19.2s\n",
      "22:\tlearn: 0.6682558\ttotal: 5.66s\tremaining: 18.9s\n",
      "23:\tlearn: 0.6683972\ttotal: 5.92s\tremaining: 18.7s\n",
      "24:\tlearn: 0.6686599\ttotal: 6.15s\tremaining: 18.4s\n",
      "25:\tlearn: 0.6681952\ttotal: 6.38s\tremaining: 18.2s\n",
      "26:\tlearn: 0.6684982\ttotal: 6.61s\tremaining: 17.9s\n",
      "27:\tlearn: 0.6692053\ttotal: 6.85s\tremaining: 17.6s\n",
      "28:\tlearn: 0.6696699\ttotal: 7.1s\tremaining: 17.4s\n",
      "29:\tlearn: 0.6699325\ttotal: 7.33s\tremaining: 17.1s\n",
      "30:\tlearn: 0.6705992\ttotal: 7.57s\tremaining: 16.9s\n",
      "31:\tlearn: 0.6709426\ttotal: 7.82s\tremaining: 16.6s\n",
      "32:\tlearn: 0.6708012\ttotal: 8.05s\tremaining: 16.3s\n",
      "33:\tlearn: 0.6709426\ttotal: 8.27s\tremaining: 16.1s\n",
      "34:\tlearn: 0.6707002\ttotal: 8.5s\tremaining: 15.8s\n",
      "35:\tlearn: 0.6715082\ttotal: 8.74s\tremaining: 15.5s\n",
      "36:\tlearn: 0.6705992\ttotal: 8.97s\tremaining: 15.3s\n",
      "37:\tlearn: 0.6725991\ttotal: 9.21s\tremaining: 15s\n",
      "38:\tlearn: 0.6729829\ttotal: 9.46s\tremaining: 14.8s\n",
      "39:\tlearn: 0.6725991\ttotal: 9.72s\tremaining: 14.6s\n",
      "40:\tlearn: 0.6734273\ttotal: 9.96s\tremaining: 14.3s\n",
      "41:\tlearn: 0.6738314\ttotal: 10.2s\tremaining: 14.1s\n",
      "42:\tlearn: 0.6741546\ttotal: 10.5s\tremaining: 13.9s\n",
      "43:\tlearn: 0.6739728\ttotal: 10.7s\tremaining: 13.7s\n",
      "44:\tlearn: 0.6741950\ttotal: 11s\tremaining: 13.4s\n",
      "45:\tlearn: 0.6750636\ttotal: 11.2s\tremaining: 13.2s\n",
      "46:\tlearn: 0.6758919\ttotal: 11.5s\tremaining: 12.9s\n",
      "47:\tlearn: 0.6757707\ttotal: 11.7s\tremaining: 12.7s\n",
      "48:\tlearn: 0.6762151\ttotal: 11.9s\tremaining: 12.4s\n",
      "49:\tlearn: 0.6774474\ttotal: 12.2s\tremaining: 12.2s\n",
      "50:\tlearn: 0.6777100\ttotal: 12.4s\tremaining: 11.9s\n",
      "51:\tlearn: 0.6786594\ttotal: 12.6s\tremaining: 11.7s\n",
      "52:\tlearn: 0.6789827\ttotal: 12.9s\tremaining: 11.4s\n",
      "53:\tlearn: 0.6804372\ttotal: 13.1s\tremaining: 11.1s\n",
      "54:\tlearn: 0.6804372\ttotal: 13.3s\tremaining: 10.9s\n",
      "55:\tlearn: 0.6809220\ttotal: 13.5s\tremaining: 10.6s\n",
      "56:\tlearn: 0.6812250\ttotal: 13.8s\tremaining: 10.4s\n",
      "57:\tlearn: 0.6813058\ttotal: 14s\tremaining: 10.2s\n",
      "58:\tlearn: 0.6811846\ttotal: 14.3s\tremaining: 9.94s\n",
      "59:\tlearn: 0.6813260\ttotal: 14.5s\tremaining: 9.68s\n",
      "60:\tlearn: 0.6816694\ttotal: 14.7s\tremaining: 9.42s\n",
      "61:\tlearn: 0.6823159\ttotal: 15s\tremaining: 9.19s\n",
      "62:\tlearn: 0.6832653\ttotal: 15.2s\tremaining: 8.95s\n",
      "63:\tlearn: 0.6840734\ttotal: 15.5s\tremaining: 8.7s\n",
      "64:\tlearn: 0.6840734\ttotal: 15.7s\tremaining: 8.44s\n",
      "65:\tlearn: 0.6846592\ttotal: 15.9s\tremaining: 8.18s\n",
      "66:\tlearn: 0.6843360\ttotal: 16.1s\tremaining: 7.95s\n",
      "67:\tlearn: 0.6846390\ttotal: 16.4s\tremaining: 7.7s\n",
      "68:\tlearn: 0.6854269\ttotal: 16.6s\tremaining: 7.46s\n",
      "69:\tlearn: 0.6858309\ttotal: 16.8s\tremaining: 7.2s\n",
      "70:\tlearn: 0.6858309\ttotal: 17.1s\tremaining: 6.97s\n",
      "71:\tlearn: 0.6865783\ttotal: 17.3s\tremaining: 6.72s\n",
      "72:\tlearn: 0.6864167\ttotal: 17.5s\tremaining: 6.48s\n",
      "73:\tlearn: 0.6868611\ttotal: 17.8s\tremaining: 6.24s\n",
      "74:\tlearn: 0.6869217\ttotal: 18s\tremaining: 6s\n",
      "75:\tlearn: 0.6870429\ttotal: 18.2s\tremaining: 5.76s\n",
      "76:\tlearn: 0.6875278\ttotal: 18.4s\tremaining: 5.51s\n",
      "77:\tlearn: 0.6881136\ttotal: 18.7s\tremaining: 5.26s\n",
      "78:\tlearn: 0.6883762\ttotal: 18.9s\tremaining: 5.02s\n",
      "79:\tlearn: 0.6888207\ttotal: 19.1s\tremaining: 4.78s\n",
      "80:\tlearn: 0.6892449\ttotal: 19.4s\tremaining: 4.55s\n",
      "81:\tlearn: 0.6898509\ttotal: 19.6s\tremaining: 4.31s\n",
      "82:\tlearn: 0.6897095\ttotal: 19.9s\tremaining: 4.07s\n",
      "83:\tlearn: 0.6902549\ttotal: 20.1s\tremaining: 3.83s\n",
      "84:\tlearn: 0.6909822\ttotal: 20.3s\tremaining: 3.58s\n",
      "85:\tlearn: 0.6910832\ttotal: 20.6s\tremaining: 3.35s\n",
      "86:\tlearn: 0.6914468\ttotal: 20.8s\tremaining: 3.11s\n",
      "87:\tlearn: 0.6916084\ttotal: 21s\tremaining: 2.87s\n",
      "88:\tlearn: 0.6919922\ttotal: 21.2s\tremaining: 2.63s\n",
      "89:\tlearn: 0.6925579\ttotal: 21.5s\tremaining: 2.38s\n",
      "90:\tlearn: 0.6928407\ttotal: 21.7s\tremaining: 2.15s\n",
      "91:\tlearn: 0.6930427\ttotal: 21.9s\tremaining: 1.91s\n",
      "92:\tlearn: 0.6935073\ttotal: 22.1s\tremaining: 1.67s\n",
      "93:\tlearn: 0.6940932\ttotal: 22.4s\tremaining: 1.43s\n",
      "94:\tlearn: 0.6944972\ttotal: 22.6s\tremaining: 1.19s\n",
      "95:\tlearn: 0.6948810\ttotal: 22.8s\tremaining: 950ms\n",
      "96:\tlearn: 0.6951840\ttotal: 23s\tremaining: 712ms\n",
      "97:\tlearn: 0.6954264\ttotal: 23.3s\tremaining: 475ms\n",
      "98:\tlearn: 0.6955881\ttotal: 23.5s\tremaining: 237ms\n",
      "99:\tlearn: 0.6956285\ttotal: 23.7s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Time: 23.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# !pip install catboost\n",
    "import catboost as cb\n",
    "start = time.time() # 시작 시간 지정\n",
    "cb_dtrain = cb.Pool(data = train_x, label = train_y) # 학습 데이터를 Catboost 모델에 맞게 변환\n",
    "cb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 100, # Number of trees, 트리 생성 개수\n",
    "            'eval_metric': 'Accuracy', # 평가 척도\n",
    "            'loss_function': 'MultiClass'} # 손실 함수, 목적 함수\n",
    "cb_model = cb.train(pool = cb_dtrain, params = cb_param) # 학습 진행\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 # 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict) * 100), \"%\") # 정확도 % 계산\n",
    "print(\"Time: %.2f\" % (time.time() - start), \"seconds\") # 코드 실행 시간 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35426047,  1.22109587,  0.44230101, ..., -0.1698448 ,\n",
       "        -0.02059177, -0.2130643 ],\n",
       "       [-0.07235138,  0.42535181,  0.20060428, ...,  0.21863604,\n",
       "         0.2719157 ,  0.25089315],\n",
       "       [-0.3315885 , -0.31862353, -0.31279765, ..., -0.29798357,\n",
       "        -0.24018767, -0.32984969],\n",
       "       ...,\n",
       "       [ 0.05304325,  0.02500267, -0.14752573, ..., -0.20741963,\n",
       "         0.12789417,  1.51166757],\n",
       "       [-0.55093666,  1.7691278 ,  0.99746884, ..., -0.3420542 ,\n",
       "        -0.49799871, -0.38136323],\n",
       "       [-0.3033724 ,  0.09352675, -0.11808658, ...,  0.65825036,\n",
       "         1.05515787, -0.20799899]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  floors  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00     1.0   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25     2.0   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00     1.0   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00     1.0   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00     1.0   \n",
       "\n",
       "   waterfront  condition  grade  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0           0          3      7      1955             0    98178  47.5112   \n",
       "1           0          3      7      1951          1991    98125  47.7210   \n",
       "2           0          3      6      1933             0    98028  47.7379   \n",
       "3           0          5      7      1965             0    98136  47.5208   \n",
       "4           0          3      8      1987             0    98074  47.6168   \n",
       "\n",
       "      long  \n",
       "0 -122.257  \n",
       "1 -122.319  \n",
       "2 -122.233  \n",
       "3 -122.393  \n",
       "4 -122.045  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "data = pd.read_csv(\"../2. Data/kc_house_data.csv\") \n",
    "data.head() # 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id', 'date', 'zipcode', 'lat', 'long'], axis = 1) # id, date, zipcode, lat, long  제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15129, 8) (6484, 8) (15129,) (6484,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['price'])) # Price를 제외한 모든 행\n",
    "X = data[feature_columns]\n",
    "y = data['price']\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) # 학습데이터와 평가데이터의 비율을 7:3\n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537729.263666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "start = time.time() # 시작 시간 지정\n",
    "lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "            'learning_rate': 0.01, # Step Size\n",
    "            'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "            'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210904.17249451784"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble의 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9546\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538375.310133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "215916.07403781536\n",
      "9640\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540050.093066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215487.73739498277\n",
      "9570\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 229\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 532551.502809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "212775.97744245254\n",
      "9596\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000881 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 530157.005949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214741.5358238521\n",
      "9437\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542015.409809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213900.0748327704\n",
      "9600\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536646.908652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213600.44257034917\n",
      "9538\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540672.176482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221747.21994598728\n",
      "9521\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000528 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537868.751999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214571.92560210422\n",
      "9606\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000437 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 240\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 534950.803226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211825.45724071778\n",
      "9580\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533883.158570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "215755.14745994387\n",
      "9503\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542151.161478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212819.45630052243\n",
      "9503\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000432 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537894.762773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215153.5946371121\n",
      "9612\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540896.539229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212696.7520235288\n",
      "9582\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536252.240135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213082.87025148343\n",
      "9550\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539870.825170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "216123.1898980709\n",
      "9556\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538302.361954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213868.9026156129\n",
      "9565\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 230\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538551.082226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218085.20742280307\n",
      "9540\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537430.587944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216507.6744533681\n",
      "9591\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 538180.885452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211669.27263569864\n",
      "9487\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 533990.481922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212920.76641183777\n",
      "9559\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537157.374050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224424.50771679275\n",
      "9603\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 232\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536167.286668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217007.6768068208\n",
      "9604\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 235\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 537594.079516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216049.0636957352\n",
      "9562\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 234\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 542638.290898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217813.6316727686\n",
      "9601\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 233\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539457.942693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "212852.28469560234\n",
      "9587\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 231\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 536156.029744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213239.3358969718\n",
      "9527\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539644.882808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "214372.10247532427\n",
      "9591\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 540095.632097\n",
      "216642.3111893549\n",
      "9585\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000451 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 237\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 541847.098883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "221088.638423943\n",
      "9596\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 236\n",
      "[LightGBM] [Info] Number of data points in the train set: 15129, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 539927.250314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/js/opt/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214460.82024552065\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "bagging_predict_result = [] # 빈 리스트 생성\n",
    "for _ in range(30):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])] # 학습 데이터의 인덱스를 리스트로 변환\n",
    "    random_data_index = np.random.choice(data_index, train_x.shape[0]) # 데이터의 1/10 크기만큼 랜덤 샘플링, // 는 소수점을 무시하기 위함\n",
    "    print(len(set(random_data_index)))\n",
    "    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index]) # 학습 데이터를 LightGBM 모델에 맞게 변환\n",
    "    lgb_param = {'max_depth': 10, # 트리 깊이\n",
    "                'learning_rate': 0.01, # Step Size\n",
    "                'n_estimators': 500, # Number of trees, 트리 생성 개수\n",
    "                'objective': 'regression'} # 파라미터 추가, Label must be in [0, num_class) -> num_class보다 1 커야한다.\n",
    "    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) # 학습 진행\n",
    " \n",
    "    predict1 = lgb_model.predict(test_x) # 테스트 데이터 예측\n",
    "    bagging_predict_result.append(predict1) # 반복문이 실행되기 전 빈 리스트에 결과 값 저장\n",
    "    print(sqrt(mean_squared_error(lgb_model.predict(test_x),test_y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산\n",
    "bagging_predict = [] # 빈 리스트 생성\n",
    "for lst2_index in range(test_x.shape[0]): # 테스트 데이터 개수만큼의 반복\n",
    "    temp_predict = [] # 임시 빈 리스트 생성 (반복문 내 결과값 저장)\n",
    "    for lst_index in range(len(bagging_predict_result)): # Bagging 결과 리스트 반복\n",
    "        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) # 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장\n",
    "    bagging_predict.append(np.mean(temp_predict)) # 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[503211.0435531791,\n",
       " 637891.5361807141,\n",
       " 954047.928798573,\n",
       " 1603968.2604410502,\n",
       " 637549.1379050323,\n",
       " 369722.27991445357,\n",
       " 705712.9717581428,\n",
       " 431993.82677551976,\n",
       " 464105.4613484621,\n",
       " 495506.11779981817,\n",
       " 631657.5995363515,\n",
       " 380207.2814799274,\n",
       " 298807.9223392497,\n",
       " 360069.2636687882,\n",
       " 345176.20698116004,\n",
       " 1308282.822561769,\n",
       " 372636.6018938016,\n",
       " 1019839.153128568,\n",
       " 318086.2801561185,\n",
       " 529472.3301326437,\n",
       " 378855.59187965293,\n",
       " 1939141.2726367398,\n",
       " 661783.2231560701,\n",
       " 540715.4817156378,\n",
       " 501413.25491812226,\n",
       " 485808.62872805167,\n",
       " 296697.9449535768,\n",
       " 246546.50086731816,\n",
       " 474352.84490094974,\n",
       " 539284.3747310798,\n",
       " 490000.6791947748,\n",
       " 472233.31501011364,\n",
       " 466215.72023013997,\n",
       " 575099.273540407,\n",
       " 378548.6115400313,\n",
       " 1030918.9556955822,\n",
       " 890993.736028023,\n",
       " 527602.9649508962,\n",
       " 355942.3204205187,\n",
       " 1598428.9062005365,\n",
       " 394519.3037608492,\n",
       " 275726.4780707739,\n",
       " 508756.0018527291,\n",
       " 341869.177833834,\n",
       " 253305.12884039653,\n",
       " 241843.26192681305,\n",
       " 329808.34836594766,\n",
       " 333696.108780273,\n",
       " 353585.81086638407,\n",
       " 569745.9790952908,\n",
       " 371222.5964724667,\n",
       " 342297.50561038684,\n",
       " 774320.0150307353,\n",
       " 337983.67779485247,\n",
       " 466692.5004389841,\n",
       " 1771482.4575672224,\n",
       " 475341.32540487143,\n",
       " 706748.1818813834,\n",
       " 334405.4319635632,\n",
       " 651348.0621636249,\n",
       " 477269.40465848945,\n",
       " 377980.3031880476,\n",
       " 300599.8505258218,\n",
       " 526253.9746284365,\n",
       " 450382.9073850445,\n",
       " 287894.36386088305,\n",
       " 387720.6591665999,\n",
       " 1506814.198227771,\n",
       " 484765.41383336333,\n",
       " 659817.6687613878,\n",
       " 431794.1093654502,\n",
       " 299112.6663704763,\n",
       " 765168.5101063751,\n",
       " 522306.44562505384,\n",
       " 511148.58053283964,\n",
       " 1272968.6466349538,\n",
       " 816341.1444081507,\n",
       " 286962.20508849993,\n",
       " 456227.56823239854,\n",
       " 921530.0390583747,\n",
       " 634280.79325918,\n",
       " 378470.3753856626,\n",
       " 651561.5925525584,\n",
       " 361818.85649964627,\n",
       " 819413.5213432751,\n",
       " 528599.8120110859,\n",
       " 522878.48441745044,\n",
       " 558326.5852362819,\n",
       " 359253.01549077086,\n",
       " 464117.736567411,\n",
       " 350284.61856434384,\n",
       " 397114.52160665375,\n",
       " 639390.7015099333,\n",
       " 1075857.66398932,\n",
       " 426756.86264937354,\n",
       " 497046.5024317336,\n",
       " 361402.86286339146,\n",
       " 306133.9788407982,\n",
       " 821938.7242200642,\n",
       " 455930.1620763879,\n",
       " 255443.01183413426,\n",
       " 929819.3008532511,\n",
       " 1001217.2394361367,\n",
       " 478501.43343861116,\n",
       " 1078993.1223734987,\n",
       " 298586.6059767863,\n",
       " 494939.4732645849,\n",
       " 484388.6077691246,\n",
       " 814003.9061659599,\n",
       " 2368083.5353246876,\n",
       " 549923.897503501,\n",
       " 323249.8984826005,\n",
       " 561630.0018523908,\n",
       " 626035.298627135,\n",
       " 551950.1036660781,\n",
       " 338222.6702030363,\n",
       " 311718.70076137944,\n",
       " 250317.3309947575,\n",
       " 322062.4330665175,\n",
       " 345176.20698116004,\n",
       " 381175.9773513368,\n",
       " 283733.126855116,\n",
       " 343240.52362398565,\n",
       " 255357.0083698806,\n",
       " 597629.1552623642,\n",
       " 654688.9287546395,\n",
       " 277639.1175899616,\n",
       " 742447.8998849267,\n",
       " 454011.3885984684,\n",
       " 428248.939283003,\n",
       " 529420.5490433312,\n",
       " 467969.2803587812,\n",
       " 411542.41520547017,\n",
       " 831830.0116601321,\n",
       " 376383.0007119138,\n",
       " 461296.2013620531,\n",
       " 383353.7607647255,\n",
       " 354280.8116386406,\n",
       " 903483.5482240601,\n",
       " 616331.5824171059,\n",
       " 517101.46042524284,\n",
       " 779477.276802324,\n",
       " 882054.6629051847,\n",
       " 404353.1428645054,\n",
       " 257638.58388954014,\n",
       " 386342.26345796324,\n",
       " 483611.77169807866,\n",
       " 241904.38526547456,\n",
       " 410195.5458956773,\n",
       " 473081.804769832,\n",
       " 572429.7832956562,\n",
       " 675485.3146955847,\n",
       " 541800.3483729275,\n",
       " 1115191.2908206128,\n",
       " 923552.219831384,\n",
       " 882390.0573668749,\n",
       " 596139.1804362838,\n",
       " 652991.1043463431,\n",
       " 588254.822015223,\n",
       " 491715.97909752134,\n",
       " 644316.2492379528,\n",
       " 368075.0991010184,\n",
       " 333696.108780273,\n",
       " 358267.31776324677,\n",
       " 365117.5244468677,\n",
       " 348354.51740345993,\n",
       " 290961.9748728044,\n",
       " 311771.3224922728,\n",
       " 449089.0446320266,\n",
       " 461568.1083538129,\n",
       " 625955.7696062729,\n",
       " 397520.5453149382,\n",
       " 465879.263919927,\n",
       " 578412.501153341,\n",
       " 427544.1587256584,\n",
       " 410652.31501901726,\n",
       " 360574.8482034042,\n",
       " 666996.8786981695,\n",
       " 348163.5824113516,\n",
       " 254504.65241863052,\n",
       " 311181.6986778828,\n",
       " 478425.3841878961,\n",
       " 529314.7043509613,\n",
       " 662707.1127399275,\n",
       " 466465.5549201591,\n",
       " 470915.0926230568,\n",
       " 273230.31614014675,\n",
       " 424428.2398324253,\n",
       " 354900.858983761,\n",
       " 351587.340702836,\n",
       " 373531.30255544366,\n",
       " 652920.5733687545,\n",
       " 1546999.1437403916,\n",
       " 1286319.792312794,\n",
       " 264480.6941298593,\n",
       " 489124.6579128138,\n",
       " 492341.41002513847,\n",
       " 1670089.2160639402,\n",
       " 453621.76809583727,\n",
       " 462217.4848752987,\n",
       " 322367.37717946037,\n",
       " 384632.00547749514,\n",
       " 521532.4977614387,\n",
       " 755868.6389818349,\n",
       " 789997.8717909758,\n",
       " 311498.532508426,\n",
       " 501413.25491812226,\n",
       " 307472.14086428465,\n",
       " 508704.7122085143,\n",
       " 1426481.850864706,\n",
       " 361402.86286339146,\n",
       " 419031.23911827453,\n",
       " 456245.80819050776,\n",
       " 361402.86286339146,\n",
       " 330082.8131411233,\n",
       " 714240.3772186551,\n",
       " 784041.694856993,\n",
       " 348829.53254386625,\n",
       " 363145.62094695505,\n",
       " 361856.11390825926,\n",
       " 1730102.8223404165,\n",
       " 534874.7169434973,\n",
       " 502999.7944728443,\n",
       " 450044.94686065096,\n",
       " 526688.3546880395,\n",
       " 754989.1489244623,\n",
       " 345133.53688789165,\n",
       " 1222479.9016553864,\n",
       " 897437.7153284028,\n",
       " 463956.5813632308,\n",
       " 356271.27319750213,\n",
       " 479886.5355750655,\n",
       " 719120.186228821,\n",
       " 298770.0619449986,\n",
       " 347948.8044936839,\n",
       " 392222.5351779444,\n",
       " 353717.5158375741,\n",
       " 361444.2856031333,\n",
       " 2454688.682141883,\n",
       " 348477.79815803183,\n",
       " 442932.12078830507,\n",
       " 463308.9305243284,\n",
       " 595612.6101515412,\n",
       " 419406.7225774462,\n",
       " 471179.28572329687,\n",
       " 309419.7266618848,\n",
       " 519190.90945156495,\n",
       " 554781.957152029,\n",
       " 715370.1622426981,\n",
       " 860209.7190670089,\n",
       " 536096.3854446785,\n",
       " 431867.1408351936,\n",
       " 743097.5182153995,\n",
       " 352835.02874962706,\n",
       " 351587.340702836,\n",
       " 527123.137713467,\n",
       " 497034.96918294765,\n",
       " 467272.557052684,\n",
       " 895248.0130729139,\n",
       " 371272.08020945167,\n",
       " 3170828.8712476795,\n",
       " 631954.3293672692,\n",
       " 761808.697347808,\n",
       " 1068636.292546089,\n",
       " 507196.89586259396,\n",
       " 669795.8777376465,\n",
       " 918362.1155653951,\n",
       " 353789.3584191582,\n",
       " 719225.4200913215,\n",
       " 436433.01345096267,\n",
       " 475168.82260021754,\n",
       " 343495.76772022224,\n",
       " 292196.6158956304,\n",
       " 437944.5338014918,\n",
       " 412988.99616526876,\n",
       " 1373772.9783383955,\n",
       " 304727.2374469501,\n",
       " 344777.04737463914,\n",
       " 526760.7505198737,\n",
       " 364821.2464243198,\n",
       " 311175.33770038397,\n",
       " 509289.5912116816,\n",
       " 371090.3709542706,\n",
       " 443505.51469451404,\n",
       " 485574.9262881165,\n",
       " 445532.33661232464,\n",
       " 371238.2703917985,\n",
       " 631893.2365370501,\n",
       " 355145.6025979035,\n",
       " 316911.55152927776,\n",
       " 787428.6543711114,\n",
       " 447522.6428168706,\n",
       " 258122.9158201413,\n",
       " 349729.8320297828,\n",
       " 652920.5733687545,\n",
       " 654616.2200143044,\n",
       " 492228.5194668112,\n",
       " 447760.3649364239,\n",
       " 451731.3203977013,\n",
       " 575237.1214773896,\n",
       " 473615.4247189441,\n",
       " 546342.970993413,\n",
       " 332354.96381530614,\n",
       " 565630.8606086184,\n",
       " 344191.28580109676,\n",
       " 793374.5299344992,\n",
       " 449089.0446320266,\n",
       " 386934.0975229143,\n",
       " 333474.14309956983,\n",
       " 327873.66291831544,\n",
       " 364799.88820603926,\n",
       " 293395.7379944367,\n",
       " 854472.8432601016,\n",
       " 1607955.1670180145,\n",
       " 963749.9733400701,\n",
       " 447534.31477943354,\n",
       " 810549.4976080722,\n",
       " 463357.1727517648,\n",
       " 795888.0979279552,\n",
       " 345374.03167060827,\n",
       " 400877.62956608296,\n",
       " 491606.56658472895,\n",
       " 264480.6941298593,\n",
       " 301103.10097958404,\n",
       " 465050.44613791426,\n",
       " 461568.1083538129,\n",
       " 568515.5783199215,\n",
       " 298758.53437654924,\n",
       " 512484.7388514749,\n",
       " 255357.0083698806,\n",
       " 661959.2810335256,\n",
       " 297454.7818744494,\n",
       " 501846.9282322999,\n",
       " 301111.3360057385,\n",
       " 392421.829171169,\n",
       " 482542.0529496131,\n",
       " 606500.1242061711,\n",
       " 472594.1030913155,\n",
       " 937421.5440762527,\n",
       " 255189.42366925042,\n",
       " 1650721.3269419305,\n",
       " 476566.6148150879,\n",
       " 438129.2406595106,\n",
       " 555628.0161823088,\n",
       " 672907.5637962096,\n",
       " 618066.0297129456,\n",
       " 343095.89940705453,\n",
       " 462131.9034117057,\n",
       " 471237.7259990385,\n",
       " 735022.225576714,\n",
       " 281584.92299085116,\n",
       " 364037.9016253526,\n",
       " 473796.9048230908,\n",
       " 632639.7783507233,\n",
       " 340879.10184530565,\n",
       " 864137.0373534847,\n",
       " 555733.4114078438,\n",
       " 926129.6682974806,\n",
       " 970908.3186958074,\n",
       " 635143.9830954202,\n",
       " 371570.31376437534,\n",
       " 797572.7154569442,\n",
       " 356138.24554949085,\n",
       " 568810.4773149615,\n",
       " 680642.7244507783,\n",
       " 332537.3857630902,\n",
       " 761240.4085496966,\n",
       " 396106.0054863557,\n",
       " 965176.9131070392,\n",
       " 387537.49601228477,\n",
       " 502832.30964226456,\n",
       " 651109.9786498856,\n",
       " 569755.410864392,\n",
       " 352190.77064536663,\n",
       " 544217.1343642584,\n",
       " 380937.7657097903,\n",
       " 340344.37518278934,\n",
       " 549251.1725762032,\n",
       " 389170.97898837464,\n",
       " 415289.97108461737,\n",
       " 373411.93765265006,\n",
       " 709440.4423682995,\n",
       " 633699.2271771954,\n",
       " 433196.4758876082,\n",
       " 462735.64339010307,\n",
       " 459695.46792552865,\n",
       " 300455.3344860739,\n",
       " 495896.64806165075,\n",
       " 427993.0516773711,\n",
       " 466543.81674043625,\n",
       " 559441.0430510669,\n",
       " 390926.9536450933,\n",
       " 380650.8568776428,\n",
       " 410672.4950244902,\n",
       " 1407028.756261077,\n",
       " 387537.49601228477,\n",
       " 345408.5028879572,\n",
       " 1218555.6834189654,\n",
       " 660217.9673407883,\n",
       " 387463.4438103751,\n",
       " 431794.1093654502,\n",
       " 475617.21665087505,\n",
       " 645824.2447782366,\n",
       " 455718.1330221993,\n",
       " 394529.88971366885,\n",
       " 434363.47265472944,\n",
       " 461568.1083538129,\n",
       " 428248.939283003,\n",
       " 485594.9309227767,\n",
       " 470954.1100166377,\n",
       " 350186.7539532046,\n",
       " 476514.4243789976,\n",
       " 617805.9798318091,\n",
       " 418455.70565444324,\n",
       " 268052.6697198555,\n",
       " 385060.6781807885,\n",
       " 463639.69492845197,\n",
       " 451061.22297341813,\n",
       " 1093460.6326875275,\n",
       " 455539.2935123444,\n",
       " 395952.0462159441,\n",
       " 565952.0109110371,\n",
       " 333557.14905400097,\n",
       " 684033.7846604522,\n",
       " 402379.1284865011,\n",
       " 474011.1703622488,\n",
       " 485892.01285908447,\n",
       " 471833.3611022019,\n",
       " 562904.3632240857,\n",
       " 334892.91735120566,\n",
       " 615226.707292651,\n",
       " 514193.86220503325,\n",
       " 706317.0006717164,\n",
       " 533988.7293036291,\n",
       " 506226.84355297475,\n",
       " 510375.5069978916,\n",
       " 526779.9956914792,\n",
       " 385859.3001156956,\n",
       " 360608.454088794,\n",
       " 355831.0113024511,\n",
       " 648027.3554367239,\n",
       " 386024.98862606863,\n",
       " 358503.2557287409,\n",
       " 275023.00996691716,\n",
       " 369554.0767240971,\n",
       " 383262.97572939185,\n",
       " 831830.0116601321,\n",
       " 2464478.2510470403,\n",
       " 442808.931866733,\n",
       " 1144637.9588924886,\n",
       " 507818.5284058557,\n",
       " 266512.5174212831,\n",
       " 484851.3837261594,\n",
       " 449218.855198871,\n",
       " 431208.32985466137,\n",
       " 667297.6894411835,\n",
       " 381318.6644692578,\n",
       " 831219.3979698529,\n",
       " 374190.0441698082,\n",
       " 278444.1586060642,\n",
       " 463926.4911877932,\n",
       " 637201.3715797258,\n",
       " 1005540.9934001452,\n",
       " 376404.89191177883,\n",
       " 705888.1101716759,\n",
       " 451228.19099218206,\n",
       " 357490.80001789593,\n",
       " 539866.9832857075,\n",
       " 994946.746007362,\n",
       " 438640.47659113153,\n",
       " 472594.1030913155,\n",
       " 350186.7539532046,\n",
       " 455671.94477897265,\n",
       " 1244741.6312721164,\n",
       " 446290.6404941942,\n",
       " 496855.91338883067,\n",
       " 390287.64984559815,\n",
       " 457226.4998191393,\n",
       " 318161.42387332104,\n",
       " 454775.5901406818,\n",
       " 302167.4760630032,\n",
       " 336389.14963638975,\n",
       " 346624.3847866794,\n",
       " 384600.13167791127,\n",
       " 443797.59966780245,\n",
       " 617193.6436988662,\n",
       " 520826.8912072208,\n",
       " 336051.07462842815,\n",
       " 759237.6925338089,\n",
       " 662059.3755281123,\n",
       " 719712.5065390127,\n",
       " 350448.441838146,\n",
       " 427544.1587256584,\n",
       " 655007.5887543325,\n",
       " 1871478.7722819413,\n",
       " 495708.8746096225,\n",
       " 684451.6501425753,\n",
       " 369929.60746176087,\n",
       " 370009.14504182973,\n",
       " 347152.16794654634,\n",
       " 696908.3198651519,\n",
       " 477269.40465848945,\n",
       " 317886.5190878116,\n",
       " 524404.8274453902,\n",
       " 349355.89521293674,\n",
       " 558834.8748183713,\n",
       " 793816.1695950598,\n",
       " 1052052.196199645,\n",
       " 329272.61446640117,\n",
       " 534385.820199189,\n",
       " 1207069.8450134168,\n",
       " 461568.1083538129,\n",
       " 818745.0905394648,\n",
       " 626324.3541083136,\n",
       " 461199.54427001934,\n",
       " 832509.253693632,\n",
       " 711390.4074306033,\n",
       " 252280.4110199513,\n",
       " 377980.3031880476,\n",
       " 287543.7546415848,\n",
       " 431185.3331085772,\n",
       " 537520.9860804691,\n",
       " 342024.8602997601,\n",
       " 494317.8678217216,\n",
       " 536248.4533303619,\n",
       " 411767.6266530761,\n",
       " 500872.09225325804,\n",
       " 357296.217976263,\n",
       " 844366.5526855417,\n",
       " 343732.201855862,\n",
       " 371798.99619641923,\n",
       " 377982.25139474496,\n",
       " 1226585.7100881601,\n",
       " 462586.7634048717,\n",
       " 288485.8698302195,\n",
       " 288230.2633368796,\n",
       " 466477.1090671158,\n",
       " 816127.4528330744,\n",
       " 476429.23870060855,\n",
       " 670026.3062861963,\n",
       " 537141.6727180864,\n",
       " 452172.3664447806,\n",
       " 980361.8555713076,\n",
       " 278100.97201247344,\n",
       " 894720.024061107,\n",
       " 637307.9908654853,\n",
       " 514543.9098515179,\n",
       " 1026722.9428338396,\n",
       " 461225.10279634484,\n",
       " 279932.7936190841,\n",
       " 848711.996721331,\n",
       " 342614.11876848905,\n",
       " 629206.2359545132,\n",
       " 1460930.3082174258,\n",
       " 347426.8737690004,\n",
       " 506319.39782972145,\n",
       " 449114.485670498,\n",
       " 368945.83057976863,\n",
       " 253138.1700684335,\n",
       " 860486.6351391112,\n",
       " 437030.32975958503,\n",
       " 290209.61885680887,\n",
       " 470904.0770169896,\n",
       " 354901.0303050443,\n",
       " 286800.6778331633,\n",
       " 491563.1165589785,\n",
       " 355726.6194035756,\n",
       " 415193.32875371637,\n",
       " 394157.93586839375,\n",
       " 480481.23623932636,\n",
       " 484273.4017013737,\n",
       " 542689.0094466469,\n",
       " 780022.5021994316,\n",
       " 333774.8612215061,\n",
       " 328809.14883010526,\n",
       " 386024.98862606863,\n",
       " 384632.00547749514,\n",
       " 328945.4736758129,\n",
       " 448689.11233477347,\n",
       " 769078.3612955433,\n",
       " 498252.67387141124,\n",
       " 840588.2497873519,\n",
       " 823316.7459940797,\n",
       " 473755.8852739369,\n",
       " 443494.6508147412,\n",
       " 483306.3890025178,\n",
       " 315816.3722479254,\n",
       " 403848.2108812594,\n",
       " 436740.8901186003,\n",
       " 319497.04809855093,\n",
       " 471443.2614094362,\n",
       " 670026.3062861963,\n",
       " 416344.3759349744,\n",
       " 408144.9965711059,\n",
       " 460105.1862933394,\n",
       " 414328.16338463663,\n",
       " 1043913.7573141977,\n",
       " 799332.4632958003,\n",
       " 755687.7023961054,\n",
       " 367783.7036818737,\n",
       " 459695.46792552865,\n",
       " 403947.8620065664,\n",
       " 567592.192298978,\n",
       " 265748.607247904,\n",
       " 393344.0865576716,\n",
       " 354430.6714487593,\n",
       " 313686.1153399388,\n",
       " 357859.559490781,\n",
       " 288947.9729062284,\n",
       " 414609.91406420444,\n",
       " 592188.4388201566,\n",
       " 309855.1917628901,\n",
       " 441309.237010712,\n",
       " 519185.9292095395,\n",
       " 491525.0809028918,\n",
       " 351890.0503183851,\n",
       " 336922.3763386581,\n",
       " 288224.04786988324,\n",
       " 538590.8130278728,\n",
       " 473728.92314685596,\n",
       " 1670937.20305417,\n",
       " 461199.54427001934,\n",
       " 2151763.524859092,\n",
       " 673878.9139746798,\n",
       " 1025742.4335018109,\n",
       " 611577.4035596906,\n",
       " 419724.0463066307,\n",
       " 456738.24458109145,\n",
       " 975121.8709659838,\n",
       " 680397.4281674444,\n",
       " 479224.62445131916,\n",
       " 472150.33057959494,\n",
       " 740712.7950262529,\n",
       " 496084.0137202579,\n",
       " 386004.0167724575,\n",
       " 402415.76134581846,\n",
       " 364903.3133734482,\n",
       " 470546.1185519303,\n",
       " 334364.5544389923,\n",
       " 737057.480300928,\n",
       " 508927.2456454366,\n",
       " 797572.7154569442,\n",
       " 697185.9250277318,\n",
       " 504251.797817439,\n",
       " 449184.5734781398,\n",
       " 477389.37380809063,\n",
       " 491027.55032610893,\n",
       " 502474.5915659855,\n",
       " 565884.7370790266,\n",
       " 309169.24971250683,\n",
       " 1069490.6218368842,\n",
       " 311132.73634165304,\n",
       " 580163.7476866606,\n",
       " 277490.1591934361,\n",
       " 1754248.2731197241,\n",
       " 841314.1754592268,\n",
       " 635084.8113017941,\n",
       " 509542.39951515605,\n",
       " 643427.556680719,\n",
       " 556635.850576418,\n",
       " 350106.76580510195,\n",
       " 529987.5249530118,\n",
       " 465833.6191315611,\n",
       " 988898.3657186796,\n",
       " 376346.9702983501,\n",
       " 501551.9416895423,\n",
       " 998739.0168828818,\n",
       " 534331.7731597285,\n",
       " 457258.9860899084,\n",
       " 449626.876195011,\n",
       " 1082876.171308716,\n",
       " 319069.8984705011,\n",
       " 632043.2483094817,\n",
       " 788845.5557143954,\n",
       " 319069.8984705011,\n",
       " 625902.4886730794,\n",
       " 594676.4337458138,\n",
       " 395457.4001425275,\n",
       " 472308.44745761435,\n",
       " 621641.6817218532,\n",
       " 433261.4094282193,\n",
       " 350448.441838146,\n",
       " 533543.8736496627,\n",
       " 277490.1591934361,\n",
       " 349855.46474503237,\n",
       " 471682.5853860134,\n",
       " 536267.4006751641,\n",
       " 449751.3155781163,\n",
       " 818432.362546826,\n",
       " 414738.63291297934,\n",
       " 298807.9223392497,\n",
       " 324773.1645440648,\n",
       " 470954.1100166377,\n",
       " 738269.9810135078,\n",
       " 744947.8085012794,\n",
       " 371272.08020945167,\n",
       " 300136.31876918173,\n",
       " 387767.6971643213,\n",
       " 273352.7747843123,\n",
       " 852533.4507594423,\n",
       " 403759.5672900986,\n",
       " 283575.0597024619,\n",
       " 338632.48301710485,\n",
       " 489108.6924458829,\n",
       " 491244.00619143783,\n",
       " 470954.1100166377,\n",
       " 1145339.6401742299,\n",
       " 1046728.7564059701,\n",
       " 1198660.1563122491,\n",
       " 433599.1426834065,\n",
       " 580072.7517824582,\n",
       " 367206.56181095535,\n",
       " 520210.1230026708,\n",
       " 359171.66173171176,\n",
       " 392109.7706875849,\n",
       " 347287.5366924629,\n",
       " 817438.084206852,\n",
       " 466692.5004389841,\n",
       " 360948.317266597,\n",
       " 397461.05873724405,\n",
       " 290928.0862227178,\n",
       " 529705.5642487473,\n",
       " 479555.0836437714,\n",
       " 427134.5416919691,\n",
       " 523540.7358132234,\n",
       " 274033.5543393546,\n",
       " 635719.8422274939,\n",
       " 523464.0803087896,\n",
       " 456227.56823239854,\n",
       " 473615.4247189441,\n",
       " 298553.68362762505,\n",
       " 361679.56522394164,\n",
       " 1136347.9088636327,\n",
       " 772599.0557261419,\n",
       " 443228.4877313332,\n",
       " 320370.3245398796,\n",
       " 351118.95062709524,\n",
       " 1026350.9289582018,\n",
       " 551531.1859434058,\n",
       " 430645.9811828271,\n",
       " 278200.71306744753,\n",
       " 378738.66046734806,\n",
       " 298101.89613242616,\n",
       " 855038.6881196913,\n",
       " 252841.99039973796,\n",
       " 321386.6539342186,\n",
       " 296621.4985579417,\n",
       " 471179.28572329687,\n",
       " 484660.9352585981,\n",
       " 595743.4576678728,\n",
       " 574218.4046397869,\n",
       " 385210.5691865694,\n",
       " 477187.35346843896,\n",
       " 566554.6268407129,\n",
       " 485024.9221763412,\n",
       " 472230.069836096,\n",
       " 616331.5824171059,\n",
       " 513616.1307865707,\n",
       " 777367.6691670187,\n",
       " 389153.0615689171,\n",
       " 313894.27722497686,\n",
       " 468238.76327858184,\n",
       " 580440.834257326,\n",
       " 471764.13553384296,\n",
       " 493156.8524639254,\n",
       " 930148.6714439305,\n",
       " 528893.2527748869,\n",
       " 747084.5650342251,\n",
       " 611879.8664946039,\n",
       " 829027.3533703828,\n",
       " 278252.60301113286,\n",
       " 481657.168300081,\n",
       " 469767.39299715933,\n",
       " 847954.9917079646,\n",
       " 887061.815601511,\n",
       " 273852.6748783852,\n",
       " 257638.58388954014,\n",
       " 490621.0509901189,\n",
       " 294288.3752475575,\n",
       " 724793.8749244276,\n",
       " 392823.01865383884,\n",
       " 478933.9257333966,\n",
       " 512593.3035607777,\n",
       " 382957.41840542445,\n",
       " 296621.4985579417,\n",
       " 294282.1936488038,\n",
       " 321737.2973964428,\n",
       " 634623.8242384436,\n",
       " 287696.77726840955,\n",
       " 369351.9369384593,\n",
       " 478723.4127694503,\n",
       " 390068.9289736117,\n",
       " 526779.9956914792,\n",
       " 368842.85490263224,\n",
       " 392109.7706875849,\n",
       " 466692.5004389841,\n",
       " 358006.6697009587,\n",
       " 1194101.0293307449,\n",
       " 614774.667922017,\n",
       " 332205.7998142091,\n",
       " 457250.4086845845,\n",
       " 768241.6499869422,\n",
       " 634664.5068401654,\n",
       " 268256.9269437671,\n",
       " 472349.3865656251,\n",
       " 392421.829171169,\n",
       " 298650.07890244946,\n",
       " 863364.2914893288,\n",
       " 475326.4676402526,\n",
       " 556049.1136107853,\n",
       " 470041.13720423705,\n",
       " 666462.7766882117,\n",
       " 567043.0188495637,\n",
       " 843590.0718799612,\n",
       " 659817.6687613878,\n",
       " 779633.2814928046,\n",
       " 380751.2992503003,\n",
       " 988577.465798318,\n",
       " 621070.4763820642,\n",
       " 770954.1322233352,\n",
       " 454312.2027891496,\n",
       " 466661.6892042658,\n",
       " 381318.6644692578,\n",
       " 466692.5004389841,\n",
       " 377980.3031880476,\n",
       " 779477.276802324,\n",
       " 523641.32839398005,\n",
       " 490427.10322769155,\n",
       " 278100.3284115679,\n",
       " 277490.1591934361,\n",
       " 299223.7750575078,\n",
       " 809349.8062680071,\n",
       " 651348.0621636249,\n",
       " 459697.54815481335,\n",
       " 358972.4522827789,\n",
       " 469923.893769037,\n",
       " 364799.88820603926,\n",
       " 652044.9900089793,\n",
       " 277907.62207148614,\n",
       " 648057.757306582,\n",
       " 530554.518278309,\n",
       " 352163.65634766774,\n",
       " 278684.5767518347,\n",
       " 484765.41383336333,\n",
       " 506006.35857239104,\n",
       " 379097.38714485546,\n",
       " 690440.4039152563,\n",
       " 336389.14963638975,\n",
       " 392823.01865383884,\n",
       " 543143.832347033,\n",
       " 301731.54091766383,\n",
       " 574835.0856473498,\n",
       " 686369.3310667739,\n",
       " 701033.3199077293,\n",
       " 489383.8804086056,\n",
       " 425759.19590824394,\n",
       " 710344.5320562999,\n",
       " 512006.98280062876,\n",
       " 651348.0621636249,\n",
       " 1554481.0529719437,\n",
       " 543820.263569834,\n",
       " 346016.6639110045,\n",
       " 697965.4360161449,\n",
       " 822052.3570852415,\n",
       " 341920.8835504756,\n",
       " 405364.46176029666,\n",
       " 378458.63982395513,\n",
       " 294335.71982015535,\n",
       " 511797.9752644528,\n",
       " 486416.4317528085,\n",
       " 411048.4348561217,\n",
       " 489077.05337157176,\n",
       " 331387.31788789766,\n",
       " 369652.3001315424,\n",
       " 253599.89113354197,\n",
       " 354280.8116386406,\n",
       " 345408.5028879572,\n",
       " 331674.2807466958,\n",
       " 344149.48373432667,\n",
       " 333350.51352054323,\n",
       " 820740.1477522274,\n",
       " 422641.231013255,\n",
       " 438006.8051819241,\n",
       " 711913.4115307885,\n",
       " 355188.8136392375,\n",
       " 252841.99039973796,\n",
       " 368280.2956847095,\n",
       " 291794.76980572776,\n",
       " 592552.8158964447,\n",
       " 303174.5138839031,\n",
       " 515614.22139886307,\n",
       " 816730.6692224323,\n",
       " 567647.1422875449,\n",
       " 563121.9028098113,\n",
       " 503027.9094585335,\n",
       " 267634.92933216545,\n",
       " 919283.8135582592,\n",
       " 637549.1379050323,\n",
       " 860209.7190670089,\n",
       " 326893.6561135944,\n",
       " 464833.021009811,\n",
       " 800358.6616943426,\n",
       " 514709.94226542016,\n",
       " 255105.5011094791,\n",
       " 1189889.5742605268,\n",
       " 311165.0071712541,\n",
       " 634831.367673611,\n",
       " 470904.0770169896,\n",
       " 753662.5173772672,\n",
       " 550877.6229135395,\n",
       " 451228.19099218206,\n",
       " 459695.46792552865,\n",
       " 447028.6117152256,\n",
       " 548721.7497088333,\n",
       " 1289197.3989352626,\n",
       " 478989.5256128779,\n",
       " 835839.7279435239,\n",
       " 485795.4268574569,\n",
       " 382091.5453441827,\n",
       " 275527.88723749435,\n",
       " 436880.52557903394,\n",
       " 559902.9766235262,\n",
       " 486478.8039471554,\n",
       " 455295.1322699673,\n",
       " 298101.89613242616,\n",
       " 364677.5154445994,\n",
       " 351990.3277124515,\n",
       " 512148.00270156044,\n",
       " 564926.3735888391,\n",
       " 287794.1015917675,\n",
       " 378239.04810476623,\n",
       " 509409.1384249184,\n",
       " 625902.4886730794,\n",
       " 976533.2017324243,\n",
       " 661864.6529757391,\n",
       " 360825.90251916,\n",
       " 779476.8732785921,\n",
       " 708992.8002296096,\n",
       " 837416.584549296,\n",
       " 354926.7698676178,\n",
       " 2251503.7012946038,\n",
       " 371389.3243843522,\n",
       " 381505.73221974197,\n",
       " 507264.683306245,\n",
       " 333600.94311607373,\n",
       " 355755.6649347222,\n",
       " 550164.5444648654,\n",
       " 658632.5095329495,\n",
       " 707095.1761920459,\n",
       " 277787.0257119358,\n",
       " 352190.77064536663,\n",
       " 547605.1394152034,\n",
       " 2258365.4631191245,\n",
       " 555283.421328683,\n",
       " 467980.78178891353,\n",
       " 911874.6552708114,\n",
       " 620262.6119815803,\n",
       " 551933.112429815,\n",
       " 786270.7546870888,\n",
       " 730271.6295040505,\n",
       " 451228.19099218206,\n",
       " 297453.6277111849,\n",
       " 440889.5419151126,\n",
       " 657054.8511254003,\n",
       " 555076.1101955317,\n",
       " 282673.91947519296,\n",
       " 298679.74811116647,\n",
       " 558221.7089327154,\n",
       " 413017.14515769365,\n",
       " 675141.6187551179,\n",
       " 491621.3967689924,\n",
       " 933349.043386772,\n",
       " 377478.47483754286,\n",
       " 294927.41027689114,\n",
       " 395262.2926644376,\n",
       " 287360.8084255023,\n",
       " 361402.86286339146,\n",
       " 341920.8835504756,\n",
       " 393882.5769983109,\n",
       " 917418.8731080447,\n",
       " 583844.7199990955,\n",
       " 365945.4969933707,\n",
       " 507966.7627820281,\n",
       " 666928.8834706951,\n",
       " 255033.45602588932,\n",
       " 531148.349831118,\n",
       " 366512.6235498465,\n",
       " 639540.2912696279,\n",
       " 650133.8210166757,\n",
       " 466096.01764397003,\n",
       " 293192.70763359475,\n",
       " 435887.51333773887,\n",
       " 350638.9262103675,\n",
       " 887089.9616584239,\n",
       " 359535.3058134515,\n",
       " 561156.9856135217,\n",
       " 413399.6938922388,\n",
       " 390287.64984559815,\n",
       " 355862.60909665376,\n",
       " 344702.26384349423,\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210400.1749177738"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(mean_squared_error(bagging_predict,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
